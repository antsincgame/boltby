# ============================================
# bolt.diy â€” Offline-Only Configuration
# ============================================
# Rename this file to .env.local and fill in the values below.
# This fork is designed for 100% offline/local operation.

# === OLLAMA (local LLM server) ===
# Don't use http://localhost:11434 due to IPv6 issues
OLLAMA_API_BASE_URL=http://127.0.0.1:11434

# Context window size (affects VRAM usage)
# DEFAULT_NUM_CTX=32768  # ~36GB VRAM
# DEFAULT_NUM_CTX=24576  # ~32GB VRAM
# DEFAULT_NUM_CTX=16384  # ~26GB VRAM (recommended for 8GB GPU)
# DEFAULT_NUM_CTX=6144   # ~24GB VRAM
DEFAULT_NUM_CTX=16384

# === LM STUDIO (local LLM server) ===
# Don't use http://localhost:1234 due to IPv6 issues
# Make sure to enable CORS in LM Studio settings
LMSTUDIO_API_BASE_URL=http://127.0.0.1:1234

# === OPENAI-LIKE (local endpoint: vLLM, TGI, text-generation-webui, etc.) ===
OPENAI_LIKE_API_BASE_URL=
OPENAI_LIKE_API_KEY=

# === POCKETBASE (local backend, auto-started) ===
POCKETBASE_ADMIN_EMAIL=admin@bolt.local
POCKETBASE_ADMIN_PASSWORD=boltadmin2024
VITE_POCKETBASE_URL=http://localhost:8090

# === SETTINGS ===
VITE_LOG_LEVEL=debug
